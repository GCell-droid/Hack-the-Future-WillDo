# ğŸ–ï¸ Deaf and Dumb Communication Model ğŸ—£ï¸

This project provides a deep learning model that helps interpret sign language using MediaPipe and TensorFlow. It captures hand gestures and converts them into text, which can then be transformed into speech using gTTS (Google Text-to-Speech). ğŸ§ ğŸ’¡

## ğŸ”§ Installation

Ensure you have Python installed. Then, install the required dependencies using:

```sh
pip install --upgrade pip
pip install --upgrade mediapipe
pip install --upgrade tensorflow
pip install --upgrade scikit-learn
pip install --upgrade matplotlib
pip install gtts
pip install flask pyngrok tensorflow numpy
```

## ğŸ“‚ Dataset Preparation

The model requires a dataset of hand gestures. The dataset is stored in the `data/` directory. You can collect images using OpenCV and MediaPipe for hand tracking. ğŸ¥ğŸ“¸

## ğŸ“Š Model Training

1. ğŸ—ï¸ The collected gesture images are preprocessed.
2. âœ‚ï¸ The dataset is split into training and testing sets.
3. ğŸ§  A TensorFlow-based deep learning model is trained on the dataset.
4. ğŸ’¾ The trained model is saved in the `models/` directory.

## âš™ï¸ Working of the Model

1. ğŸ“· The camera captures hand gestures.
2. ğŸ–ï¸ MediaPipe processes the hand landmarks.
3. ğŸ¤– The deep learning model predicts the corresponding letter/word.
4. ğŸ“ The detected text is displayed on the screen.
5. ğŸ”Š The text is converted into speech using gTTS.

## ğŸ” Basic Functionality Implementation

### 1. âœ‹ Hand Tracking using MediaPipe
- We use MediaPipe's Hand Tracking module to detect and track hand landmarks in real time.
- Each detected hand landmark is represented as a set of coordinates.

### 2. ğŸ—‚ï¸ Data Collection and Preprocessing
- OpenCV captures hand images and stores them as training samples.
- The hand landmarks extracted by MediaPipe are used as features.

### 3. ğŸ‹ï¸ Model Training with TensorFlow
- A neural network is trained on the extracted features to classify gestures.
- The model learns to recognize different hand signs and map them to corresponding text labels.

### 4. ğŸ”‰ Text-to-Speech Conversion using gTTS
- The predicted text is passed to Google Text-to-Speech (gTTS).
- gTTS converts the text into an audio file that is played to communicate the recognized sign.

### 5. ğŸŒ Flask-Based Web Interface
- A simple web interface is created using Flask.
- The user can interact with the model through a webcam-based gesture recognition system.
- Ngrok is used to expose the Flask app for remote access.

## â–¶ï¸ Running the Model

To run the model, execute the following script:

```sh
python app.py
```

This will start a Flask server where you can interact with the model.

## âœ¨ Features
- ğŸ–ï¸ Real-time hand gesture detection
- ğŸ“ Text conversion of sign language
- ğŸ”Š Speech output for better communication
## ğŸ¯ Accuracy
![alt text](accuracy1.png)
![alt text](accuracy2.png)

## ğŸš€ Deployment
The model can be deployed using Flask and Ngrok for online access.

## ğŸ‘¥ Authors
- Omni
- Chitransh Kumar
- Nitesh Parihar
- Siddharth Nimbalakar
- Arpita